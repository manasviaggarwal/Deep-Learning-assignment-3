# -*- coding: utf-8 -*-
"""dl3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13-R-OGX8RwhclQwO671ShrXnbgg5qIq6
"""

from __future__ import print_function
from functools import reduce
import json
import os
import re
import tarfile
import tempfile

import numpy as np
np.random.seed(1337)

import keras
import keras.backend as K
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import concatenate
from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed
from keras.layers.embeddings import Embedding
from keras.layers.normalization import BatchNormalization
from keras.layers.wrappers import Bidirectional
from keras.models import Model
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from keras.regularizers import l2
from keras.utils import np_utils
import numpy as np
import pandas as pd
from keras.layers import concatenate
from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed
import keras
from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer
from keras.utils import np_utils
import json
# from __future__ import print_function
from functools import reduce
import json
import os
import re
# import nltk
# from nltk.corpus import stopwords
from sklearn.preprocessing import LabelEncoder
from keras.layers.embeddings import Embedding
from sklearn.linear_model import LogisticRegression
from keras.models import Model
from scipy.sparse import coo_matrix, hstack

from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from keras.utils import np_utils
import numpy as np
import pickle
import numpy as np
import pickle as pkl
import networkx as nx
import scipy.sparse as sp
from scipy.sparse.linalg.eigen.arpack import eigsh
import sys
from keras.models import load_model
from keras.models import model_from_json


# from google.colab import drive
# drive.mount('/content/gdrive')
# pre_comp_weight='glove.840B.300d-char.txt'  #'/content/gdrive/My Drive/dl3_data/glove.840B.300d-char.txt' '/content/gdrive/My Drive/glove.42B.300d.txt' #
dim=300
batch=512
pat = 4 # 8
MAX_EPOCHS = 1#10
dropoutt = 0.2
L2 = 4e-6
ACTIVATION = 'relu'
OPTIMIZER = 'rmsprop'
glv = 'globe'
B = {'contradiction': 0, 'neutral': 1, 'entailment': 2}

def tokens(parse):
    return parse.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').split()


def examples(fn):
    maj=True
    for i, l in enumerate(open(fn)):
        dataset = json.loads(l)
        label = dataset['gold_label']
        test_s1 = ' '.join(tokens(dataset['sentence1_binary_parse']))
        test_s2 = ' '.join(tokens(dataset['sentence2_binary_parse']))
        if maj and label == '-':
            continue
        yield (label, test_s1, test_s2)

def load_data():
  
  raw_dataset = list(examples("snli_1.0/snli_1.0_train.jsonl"))
  raw_dataset1 = list(examples("snli_1.0/snli_1.0_test.jsonl"))
  # raw_dataset2 = list(examples("snli_1.0/snli_1.0_dev.jsonl"))
  a1 = [test_s1 for _, test_s1, test_s2 in raw_dataset]
  a2 = [test_s2 for _, test_s1, test_s2 in raw_dataset]
  Y = np.array([B[l] for l, test_s1, test_s2 in raw_dataset])
  Y = np_utils.to_categorical(Y, len(B))
  trainset=[a1, a2, Y]

  a1 = [test_s1 for _, test_s1, test_s2 in raw_dataset1]
  a2 = [test_s2 for _, test_s1, test_s2 in raw_dataset1]
  Y = np.array([B[l] for l, test_s1, test_s2 in raw_dataset1])
  Y = np_utils.to_categorical(Y, len(B))
  testset=[a1, a2, Y]

  # a1 = [test_s1 for _, test_s1, test_s2 in raw_dataset2]
  # a2 = [test_s2 for _, test_s1, test_s2 in raw_dataset2]
  # Y = np.array([B[l] for l, test_s1, test_s2 in raw_dataset2])
  # Y = np_utils.to_categorical(Y, len(B))
  # valset=[a1, a2, Y]
  return trainset,testset

# def intialize_weights():
#   embeddings_index = {}
#   f = open(pre_comp_weight)
#   for line in f:
#     values = line.split(' ')
#     word = values[0]
#     coefs = np.asarray(values[1:], dtype='float32')
#     embeddings_index[word] = coefs
#   f.close()
  
#   # prepare embedding matrix
#   embd = np.zeros((unique_words, 300))
#   for word, i in tkn.word_index.items():
#     vec = embeddings_index.get(word)
#     if vec is not None:
#       embd[i] = vec
#   np.save(glv, embd)

# def data_processing(unique_words,embd):
#   embed = Embedding(unique_words, 300, weights=[embd], input_length=42, trainable=False)
#   arguments ={}
#   arguments['output_dim']=300
#   arguments['dropout_W']=dropoutt
#   arguments['dropout_U']=dropoutt
#   # arguments = dict(output_dim=300, dropout_W=dropoutt, dropout_U=dropoutt)
#   # emnd_sum = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(300, ))
#   trans = TimeDistributed(Dense(300, activation='relu'))
#   sent1 = Input(shape=(42,), dtype='int32')
#   sent2 = Input(shape=(42,), dtype='int32')
#   prem = embed(sent1)
#   hypo = embed(sent2)
#   prem = trans(prem)
#   hypo = trans(hypo)
#   return (arguments,prem,hypo,sent1,sent2)

# to_seq = lambda X: pad_sequences(tkn.texts_to_sequences(X), maxlen=42)
# prepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])
# trainset = prep(trainset)
# validation = prep(validation)
# test = prep(test)



# def model_init(arguments,prem,hypo,sent1,sent2):
#   rnn_kwargs = dict(output_dim=300, dropout_W=0.2, dropout_U=0.2)

#   RNN = lambda *args, **kwargs: Bidirectional(recurrent.LSTM(*args, **kwargs))
#   # if RNN and 1 > 1:
#   #   for l in range(1 - 1):
#   #     rnn = RNN(return_sequences=True, **arguments)
#   #     prem = BatchNormalization()(rnn(prem))
#   #     hypo = BatchNormalization()(rnn(hypo))
#   # rnn = emnd_sum if not RNN else RNN(return_sequences=False, **arguments)

#   rnn = RNN(return_sequences=False, **rnn_kwargs)
#   # print("--------------------------------------------------------------------------")
#   prem = rnn(prem)
#   # print("--------------------------------------------------------------------------")
#   prem = BatchNormalization()(prem)
#   # print("--------------------------------------------------------------------------")
#   hypo = rnn(hypo)
#   hypo = BatchNormalization()(hypo)
#   s1_s2_combined=concatenate([prem, hypo])
#   s1_s2_combined = Dropout(dropoutt)(s1_s2_combined)
#   for i in range(3):
#     s1_s2_combined = Dense(2 * 300, activation='relu', W_regularizer=l2(L2) if L2 else None)(s1_s2_combined)
#     s1_s2_combined = Dropout(dropoutt)(s1_s2_combined)
#     s1_s2_combined = BatchNormalization()(s1_s2_combined)



#   pred = Dense(len(B), activation='softmax')(s1_s2_combined)
#   model = Model(input=[sent1, sent2], output=pred)
#   model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])
#   model.summary()

#   return model

def model_test(test):
  json_file = open('deep_model_json.json', 'r') 
  loaded_model_json = json_file.read()
  json_file.close()
  loaded_model = model_from_json(loaded_model_json)
# load weights into new model
  loaded_model.load_weights("deep_model_weights.h5")
  print("Loaded model from disk")
  loaded_model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])
  
# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
  loss,acc = loaded_model.evaluate([test[0], test[1]], test[2], batch_size=512)


  # load model
  
  print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))
  p=loaded_model.predict([test[0], test[1]])

  prediction=np.argmax(p,axis=1)
  # print(prediction[0])
  # print(len(prediction))
# print(result*100)

  LABELS = B#{'contradiction': 0, 'neutral': 1, 'entailment': 2}
  file=open("deep_model.txt","w+")
  fizzbuzz=[]
  for f in prediction:
      if f==0:
          fizzbuzz.append("contradiction")

      elif f==1:
          fizzbuzz.append("neutral")

      elif f==2 :

          fizzbuzz.append("entailment")

  for i in fizzbuzz:
      file.write(str(i))
      file.write("\n")


# def model_training(model,trainset,testset,valset):
  
#   _, tmpfn = tempfile.mkstemp()
#   # print(trainset[0].shape)
#   # print(trainset[1].shape)
#   # print(trainset[2].shape)
#   # exit()
#   callbacks = [EarlyStopping(patience=4), ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]
#   model.fit([trainset[0], trainset[1]], trainset[2], batch_size=512, nb_epoch=42, validation_data=([valset[0], valset[1]], valset[2]), callbacks=callbacks)
#   loss, acc = model.evaluate([testset[0], testset[1]], testset[2], batch_size=512)


#   print('Test loss:  ',loss, 'test accuracy:  ',acc)
#   pred=model.predict_classes(testx, verbose=0)
#   print(pred[0])
#   print("saving the model")
#   print(np.array(pred).shape)
  # model.save("d_l.h5")



def test_deep():#if __name__ == "__main__":
  trainset,testset=load_data()
  tkn = Tokenizer(lower=False, filters='')
  tkn.fit_on_texts(trainset[0] + trainset[1])
  unique_words = len(tkn.word_counts) + 1
  a=trainset[0]
  b=trainset[1]
  c=trainset[2]
  a=pad_sequences(tkn.texts_to_sequences(a),maxlen=42)
  b=pad_sequences(tkn.texts_to_sequences(b),maxlen=42)
  trainset=[a,b,c]

  a=testset[0]
  b=testset[1]
  c=testset[2]
  a=pad_sequences(tkn.texts_to_sequences(a),maxlen=42)
  b=pad_sequences(tkn.texts_to_sequences(b),maxlen=42)
  testset=[a,b,c]
  # exit()
  # if not os.path.exists(glv + '.npy'):
    # print("innnnnnn")
    # intialize_weights() 
  # print('Using GloVe Embeddings')
  # embd = np.load(glv + '.npy') 
  # arguments,prem,hypo,sent1,sent2=data_processing(unique_words,embd)

  model_test(testset)
  # model=model_init(arguments,prem,hypo,sent1,sent2)
  # model_training(model,trainset,testset,valset)




